{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0Y77rtTIkqG"
      },
      "outputs": [],
      "source": [
        "#Install packages here, please note that this notebook has been optimized for Google Colabs\n",
        "!pip install datasets torch einops\n",
        "!pip install transformers>=4.28 # Ensure transformers version is at least 4.28\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1NYYiCgJBQs"
      },
      "outputs": [],
      "source": [
        "#Import everything here\n",
        "import pandas as pd\n",
        "import random\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, BertModel, AutoConfig\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, load_metric\n",
        "from datasets import Dataset\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h46BJacgojKu"
      },
      "outputs": [],
      "source": [
        "# Load DNABERT model and tokenizer\n",
        "model_name = \"zhihan1996/DNA_bert_6\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki6ZehEcR6Wi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Tx8tHLSGna"
      },
      "outputs": [],
      "source": [
        "# Load E. coli dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/fullseq_microbigge_ecoli.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-cNoxiwiKK7"
      },
      "outputs": [],
      "source": [
        "df['Type'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13g7YP5iGDFD"
      },
      "outputs": [],
      "source": [
        "df2 = df.head(1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVPRwUbCGGWu"
      },
      "outputs": [],
      "source": [
        "df2['Type'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrk7HkTCTAjR"
      },
      "outputs": [],
      "source": [
        "# Map labels to integers: AMR -> 1, anything else -> 0\n",
        "df2['Label'] = df2['Type'].apply(lambda x: 1 if x == 'AMR' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "XcfbWeT6iVms",
        "outputId": "e9b68b91-3518-4ba1-cc26-cc62572a30f9"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s_kYJY3N4xK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def kmer_shift(sequence, k=6):\n",
        "    \"\"\"\n",
        "    Shifts the k-mers of a sequence by one position.\n",
        "    \"\"\"\n",
        "    shifted_sequences = []\n",
        "    for i in range(1, k):\n",
        "        shifted_sequence = sequence[i:] + sequence[:i]\n",
        "        shifted_sequences.append(shifted_sequence)\n",
        "    return shifted_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69hBHcd4N6km"
      },
      "outputs": [],
      "source": [
        "def random_mutation(sequence, num_mutations=1):\n",
        "    \"\"\"\n",
        "    Introduces random mutations in the sequence.\n",
        "    \"\"\"\n",
        "    sequence = list(sequence)\n",
        "    for _ in range(num_mutations):\n",
        "        pos = random.randint(0, len(sequence) - 1)\n",
        "        sequence[pos] = random.choice(['a', 'c', 'g', 't'])\n",
        "    return ''.join(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDC7JdduN_0m"
      },
      "outputs": [],
      "source": [
        "def reverse_complement(sequence):\n",
        "    \"\"\"\n",
        "    Generates the reverse complement of a DNA sequence.\n",
        "    \"\"\"\n",
        "    complement = {'a': 't', 't': 'a', 'c': 'g', 'g': 'c'}\n",
        "    return ''.join(complement[base] for base in reversed(sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1MY1HPPOPGu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def augment_data(sequences, labels, k=6, num_mutations=1):\n",
        "    \"\"\"\n",
        "    Augments the data by k-mer shifting and introducing random mutations.\n",
        "    \"\"\"\n",
        "    augmented_sequences = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for seq, label in zip(sequences, labels):\n",
        "        augmented_sequences.append(seq)  # Add original sequence\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # K-mer shifting\n",
        "        shifted_seqs = kmer_shift(seq, k)\n",
        "        augmented_sequences.extend(shifted_seqs)\n",
        "        augmented_labels.extend([label] * len(shifted_seqs))\n",
        "\n",
        "        # Introduce mutations (single point mutations)\n",
        "        for _ in range(num_mutations):\n",
        "            mutated_seq = list(seq)  # Convert to list for mutability\n",
        "            mutation_index = random.randint(0, len(seq) - 1)\n",
        "            valid_nucleotides = \"acgt\".replace(seq[mutation_index], \"\")\n",
        "            mutated_seq[mutation_index] = random.choice(valid_nucleotides)\n",
        "            augmented_sequences.append(\"\".join(mutated_seq))\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return augmented_sequences, augmented_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ejAci2OulRE"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')  # Adjust 'average' as needed\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vaR1E6QrRu"
      },
      "outputs": [],
      "source": [
        "# Function to load, clean, split, and augment data\n",
        "def load_clean_split_augment_data(df, test_size=0.2):\n",
        "\n",
        "    # Drop rows with missing sequences\n",
        "    df.dropna(subset=['full_sequence'], inplace=True)\n",
        "\n",
        "    # Train-test split\n",
        "    train_data, test_data = train_test_split(df, test_size=test_size, random_state=42, stratify=df['Label'])\n",
        "\n",
        "    # Augment training data\n",
        "    augmented_sequences, augmented_labels = augment_data(train_data['full_sequence'].tolist(), train_data['Label'].tolist())\n",
        "\n",
        "    # Create new DataFrame for augmented training data\n",
        "    augmented_train_data = pd.DataFrame({'full_sequence': augmented_sequences, 'Label': augmented_labels})\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    train_dataset = Dataset.from_pandas(augmented_train_data)\n",
        "    test_dataset = Dataset.from_pandas(test_data)\n",
        "\n",
        "    return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7diPUmXpSRrv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load, clean, split, and augment dataset\n",
        "train_dataset, test_dataset = load_clean_split_augment_data(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALAg_2qSG1Uo"
      },
      "outputs": [],
      "source": [
        "train_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uItDTvpNG4K2"
      },
      "outputs": [],
      "source": [
        "test_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DALZ9EdnPT7i"
      },
      "outputs": [],
      "source": [
        "output_dir = './results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uory6NkbnI6D"
      },
      "outputs": [],
      "source": [
        "# Apply the tokenizer function to the datasets\n",
        "train_encodings = tokenizer(train_dataset['full_sequence'], truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_dataset['full_sequence'], truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIYyIX86l9uJ"
      },
      "outputs": [],
      "source": [
        "train_labels = [example['Label'] for example in train_dataset]\n",
        "test_labels = [example['Label'] for example in test_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbVVy83hN9K1"
      },
      "outputs": [],
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d89w7msAthsQ"
      },
      "outputs": [],
      "source": [
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # Use clone().detach()\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLGkzGaAtlFV"
      },
      "outputs": [],
      "source": [
        "# Wrap the tokenized data in Dataset objects (using datasets.Dataset)\n",
        "train_dataset = datasets.Dataset.from_dict({**train_encodings, 'labels': train_labels})\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n",
        "\n",
        "val_dataset = datasets.Dataset.from_dict({**test_encodings, 'labels': test_labels})\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjMKD9XHtr3w"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer with Dataset objects\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Use Dataset object\n",
        "    eval_dataset=val_dataset,    # Use Dataset object\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "RzpzYUt4tvFb",
        "outputId": "88fb325b-ddb3-463f-da0c-6b151423bab8"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"Evaluation result: {eval_result}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
